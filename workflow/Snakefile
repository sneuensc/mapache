"""
Author: Samuel Neuenschwander, Diana Ivette Cruz, Lucas Anchieri, Anna-Sapfo Malaspinas 
Affiliation: DBC, UNIL
Aim: Map ancient DNA libraries to a reference genome
Date: 28/10/2019
"""

import pandas as pd
import numpy as np
import itertools

## functions to read the config file
def get_param1(key, default):
    return config.get(key, default)

def get_param2(key1, key2, default):
    return config.get(key1, {}).get(key2, default)

def get_param3(key1, key2, key3, default):
    return config.get(key1, {}).get(key2, {}).get(key3, default)    

## convert string to boolean
def str2bool(v):
    return str(v).lower() in ("yes", "true", "t", "1")

##########################################################################################
## read the config file
configfile: "config/config.yaml"

## default values
SAMPLES               = get_param1("SAMPLES", "samples.txt")
email                 = get_param1("email", "")
delim                 = get_param1("delim", "\s+")
mapper                = get_param2("mapping", "mapper", "bwa_aln") 

## mapping options
run_adapter_removal   = str2bool(get_param2("adapterremoval", "run", True))
run_remove_duplicates = str2bool(get_param2("markduplicates", "run", True))
run_mapDamage_rescale = str2bool(get_param2("mapdamage", "run_rescale", False)) 
run_realign           = str2bool(get_param2("realign", "run", True)) 
run_compute_md        = str2bool(get_param2("compute_md", "run", True))  

## stats
run_damage            = get_param2("stats", "run_damage", 'False')
run_depth             = str2bool(get_param2("stats", "run_depth", 'False'))
run_bammds            = str2bool(get_param2("stats", "run_bammds", 'False'))

## retry failed jobs
memory_increment_ratio  = float(get_param1("memory_increment_ratio", 1)) 
runtime_increment_ratio = float(get_param1("runtime_increment_ratio", 0))

## software versions
module_samtools       = get_param2("ENVMODULES", "samtools", "")
module_bowtie2        = get_param2("ENVMODULES", "bowtie2", "")
module_bwa            = get_param2("ENVMODULES", "bwa", "")
module_picard         = get_param2("ENVMODULES", "picard", "")
module_gatk3          = get_param2("ENVMODULES", "gatk3", "")
module_fastqc         = get_param2("ENVMODULES", "fastqc", "")
module_r              = get_param2("ENVMODULES", "r", "")
module_adapterremoval = get_param2("ENVMODULES", "adapterremoval", "")
module_mapdamage      = get_param2("ENVMODULES", "mapdamage", "")
module_multiqc        = get_param2("ENVMODULES", "multiqc", "")

def RequiredModules():
    """Returns list of required module names."""
    return [ module_bwa,
            module_bowtie2,
            module_samtools,
            module_gatk3,
            module_picard,
            module_fastqc,
            module_adapterremoval,
            module_mapdamage,
            module_multiqc,
            module_r
        ]

        
##########################################################################################
## some test to verify that the minimal requirements are met

## is at least a reference genome defined
GENOME = config['GENOME'].keys()
if len(GENOME) == 0:
    print("ERROR: Reference genome is not specified (parameter GENOME)!")
    os._exit(0)

## test if for each reference a fasta file is defined
for i in GENOME:
    if 'fasta' not in config['GENOME'][i].keys():
        print("ERROR: Parameter GENOME required, but not set!")
        os._exit(0)

## if the bam files have to be rescaled mapDamage has to be run
if run_mapDamage_rescale and get_param2("stats", "run_damage", "False") != 'mapDamage':
    print("ERROR: To use 'mapdamage:run_rescale' the parameter 'stats:run_damage' has to be set to 'mapDamage'!")
    os._exit(0)


## if one wants to extract the duplicates, MarkDuplicate has to mark the duplicates in order to extract them
extract_duplicates = str2bool(get_param2("markduplicates", "extract_duplicates", "False"))
if  extract_duplicates and 'REMOVE_DUPLICATES=true' in get_param2("markduplicates", "params", ""):
    print("ERROR: To use 'markduplicates:extract_duplicates' the parameter 'markduplicates:params' has NOT to contain 'REMOVE_DUPLICATES=true'!")
    os._exit(0)

##########################################################################################
## read the sample file
# SAMPLES="samples.txt"
db = pd.read_csv(SAMPLES, sep=delim)


## check number of columns and column names
if len(db.columns) == 6:
    paired_end=0        ## single-end library
    if not set(['ID','Data', 'MAPQ', 'LB', 'PL', 'SM']).issubset(db.columns):
        print("ERROR: The column names in the sample file are wrong! Expected are for single-end reads: ID, Data, MAPQ, LB, PL, SM")
        os._exit(0)    
elif len(db.columns) == 7:
    adaptrem_params = config["adaptrem_params"] if "adaptrem_params" in config.keys() else ''
    if '--collapse' in adaptrem_params:
        paired_end=1    ## paired-end libraries, which are collapsed with adapterremoval
    else:
        paired_end=2    ## paired-end libraries, which are mapped as paired-end

    if not set(['ID','Data1', 'Data2', 'MAPQ', 'LB', 'PL', 'SM']).issubset(db.columns):
        print("ERROR: The column names in the sample file are wrong! Expected are for paired-end reads: ID, Data1, Data2, MAPQ, LB, PL, SM")
        os._exit(0)    
else: 
    print("ERROR: The number of columns in the sample file is wrong (got "+len(db.columns)+" columns)!")
    os._exit(0)


## check if all IDs per LB and SM are unique
all_fastq = db.groupby(['ID','LB','SM'])['ID'].agg(['count']).reset_index()
if max(all_fastq['count']) > 1:
    print("ERROR: The ID's have to be unique within a library and sample!")
    os._exit(0)


## dataframe to nested dict
samples = {}
cols = [e for e in list(db.columns) if e not in ('SM', 'LB', 'ID')]
for index, row in db.iterrows():
    ## if key not present add new dict
    if row['SM'] not in samples:
        samples[row['SM']] = {}
    if row['LB'] not in samples[row['SM']]:
        samples[row['SM']][row['LB']] = {}
    if row['ID'] not in samples[row['SM']][row['LB']]:
        samples[row['SM']][row['LB']][row['ID']] = {}
    
    ## add all remaining columns to this dict
    for col in cols:
        samples[row['SM']][row['LB']][row['ID']][col] = row[col]




## Building dictionnary for libraries
all_libraries = db.groupby(['LB','SM'])['ID'].agg(['count']).reset_index()


## get incremental memory allocation when jobs fail
## 'startStr' is the first memory allocation in GB
## input is in GB; output in MB; default is 2GB, but can be changed by a rule

## get incremental memory allocation when jobs fail
## 'start' is the first memory allocation in GB (default 4GB)
## input is in GB; output is in MB;
## global variable memory_increment_ratio defines by how much (ratio) the memory is increased if not defined specifically
def get_memory_alloc(module, attempt, default=2):
    mem_start = int(config.get(module, {}).get("mem", default))
    mem_incre = int(config.get(module, {}).get("mem_increment", memory_increment_ratio*mem_start))
    return int(1024 * ((attempt-1) * mem_incre + mem_start))
    
    
def convert_time(seconds): 
    day = seconds // (24 * 3600) 
    seconds = seconds % (24 * 3600) 
    hour = seconds // 3600
    seconds %= 3600
    minutes = seconds // 60
    seconds %= 60  
    return "%d-%02d:%02d:%02d" % (day, hour, minutes, seconds) 
    
    
## get incremental time allocation when jobs fail
## 'start' is the first time allocation in hours (default 12h)
## input is in hours; output is in minutes;
## global variable runtime_increment_ratio defines by how much (ratio) the time is increased if not defined specifically
def get_runtime_alloc(module, attempt, default=12):
    time_start = int(config.get(module, {}).get("time", default))
    time_incre = int(config.get(module, {}).get("time_increment", runtime_increment_ratio*time_start))
    return int(60 * ((attempt-1) * time_incre + time_start))
    #return convert_time(60*60 * ((attempt-1) * time_incre + time_start))
    
    
## get the number of threads of the given parameter
def get_threads(module, default=1):
    return int(config.get(module, {}).get("threads", default))


## define how to quantify the deamination pattern
def get_damage(run_damage):
    files=[]
    if run_damage == 'bamdamage':
        for genome in GENOME:
            files+=[("results/03_sample/{SM}/{LB}/library_bamdamage/{LB}.{genome}.dam.svg").
                format(SM=row['SM'], LB=row['LB'], genome=genome) for index, row in all_libraries.iterrows()]
    elif run_damage == 'mapDamage':
        for genome in GENOME:
            files+=[("{SM}/{LB}/library_mapDamage/{LB}.{genome}_results_mapDamage/Fragmisincorporation_plot.pdf").
                format(SM=row['SM'], LB=row['LB'], genome=genome) for index, row in all_libraries.iterrows()]

    return (files)
    

    
##########################################################################################
include : "rules/Snakefile_index.smk"
include : "rules/Snakefile_stats.smk"
include : "rules/Snakefile_fastq.smk"
include : "rules/Snakefile_library.smk"
include : "rules/Snakefile_sample.smk"
##########################################################################################
##########################################################################################
localrules: all, dag, mapping, stats, check_software_modules ## executed locally on a cluster

## rules all
rule all:
    """
    Computes all
    """
    input:
       # "logs/check_software_modules.done",
        expand("results/03_sample/03_final_sample/01_bam/{id_sample}.{id_genome}.bam", id_sample=samples.keys(), id_genome=GENOME),
        expand("results/03_sample/03_final_sample/01_bam_low_qual/{id_sample}.{id_genome}.bam", id_sample=samples.keys(), id_genome=GENOME),
        expand("results/03_sample/03_final_sample/01_bam_duplicate/{id_sample}.{id_genome}.bam", id_sample=samples.keys(), id_genome=GENOME) if extract_duplicates else [],
        expand("results/03_sample/04_stats/01_summary/sample_stats.{id_genome}.csv", id_genome=GENOME),
        expand("results/03_sample/04_stats/01_summary/1_nb_reads.{id_genome}.png", id_genome=GENOME),
        expand("results/03_sample/04_stats/01_summary/2_mapped.{id_genome}.png", id_genome=GENOME),
        expand("results/03_sample/04_stats/01_summary/3_endogenous.{id_genome}.png", id_genome=GENOME),
        expand("results/03_sample/04_stats/01_summary/4_duplication.{id_genome}.png", id_genome=GENOME),
        #expand("results/03_sample/04_stats/{id_sample}.{id_genome}_depth.txt", id_sample=samples.keys(), id_genome=GENOME) if str2bool(get_param3("stats", "library", "depth", "False")) else [],
        expand("results/03_sample/03_final_sample/01_bam/{id_sample}.{id_genome}_depth.txt", id_sample=samples.keys(), id_genome=GENOME) if str2bool(get_param3("stats", "sample", "depth", "False")) else []
       # expand("results/03_sample/04_stats/02_depth/{id_sample}.{id_genome}_depth.txt", id_sample=samples.keys(), id_genome=GENOME)  #if bool(config.get("stats", {}).get("sample", {}).get("depth", "False"))


## rules dag (just pipeline)
rule dag:
    """
    Computes just the mapping (used to get a nice DAG)
    """
    input:
        expand("results/03_sample/03_final_sample/01_bam/{id_sample}.{id_genome}.bam", id_sample=samples.keys(), id_genome=GENOME),


rule mapping:
    """
    Computes just what is needed to get the final mapping (without any stats)
    """
    input:
       # "logs/check_software_modules.done",
        expand("results/03_sample/03_final_sample/01_bam/{id_sample}.{id_genome}.bam", id_sample=samples.keys(), id_genome=GENOME),
        expand("results/03_sample/03_final_sample/01_bam_low_qual/{id_sample}.{id_genome}.bam", id_sample=samples.keys(), id_genome=GENOME),
        expand("results/03_sample/03_final_sample/01_bam_duplicate/{id_sample}.{id_genome}.bam", id_sample=samples.keys(), id_genome=GENOME) if extract_duplicates else [],
        expand("results/03_sample/04_stats/01_summary/sample_stats.{id_genome}.csv", id_genome=GENOME)

rule stats:
    """
    Computes all statistics
    """
    input:
        expand("results/03_sample/04_stats/01_summary/sample_stats.{id_genome}.csv", id_genome=GENOME),
        #expand("results/04_stats/sample_depth.{id_genome}.csv", id_genome=GENOME) if run_depth else [],
        #get_damage(run_damage),
        expand("results/03_sample/04_stats/01_summary/1_nb_reads.{id_genome}.png", id_genome=GENOME),
        expand("results/03_sample/04_stats/01_summary/2_mapped.{id_genome}.png", id_genome=GENOME),
        expand("results/03_sample/04_stats/01_summary/3_endogenous.{id_genome}.png", id_genome=GENOME),
        expand("results/03_sample/04_stats/01_summary/4_duplication.{id_genome}.png", id_genome=GENOME)

    
onsuccess:
    print("Wow, \'snakemake_aDNA_mapping\' finished successfully!")
    if email != "":
        print(f"sending email to {email}")
        shell('mail -s "Snakemake workflow \'snakemake_aDNA_mapping\' finished successfully" {email} < {log}')

onerror:
    print("An error occurred, stored to 'snakemake.run.log'")
    shell("cat {log} > snakemake.run.log")
    if email != "":
        print(f"sending email to {email}")
        shell('mail -s "Snakemake workflow \'snakemake_aDNA_mapping\' finished with error(s)" {email} < {log}')
        
##########################################################################################


rule check_software_modules:
    """
    Checks if software modules are available
    """
    output: 
        touch("logs/check_software_modules.done")
    params: 
        modules=RequiredModules()
    message: "--- Loading required modules"
    threads: 1
    shell: "{params.modules}"




##########################################################################################
##########################################################################################
