"""
Author: Samuel Neuenschwander, Diana Ivette Cruz, Lucas Anchieri, Anna-Sapfo Malaspinas 
Affiliation: DBC, UNIL
Aim: Map ancient DNA libraries to a reference genome
Date: 28/10/2019
"""


##########################################################################################
import logging

LOGGER = logging.getLogger("snakemake.logging")
# logging.basicConfig(filename='example.log', encoding='utf-8', level=logging.DEBUG)
# logging.basicConfig(format='%(levelname)s %(message)s')


## read the config file
configfile: "config/config.yaml"


# validate(config)


include: "rules/common.smk"


## default values
sample_file = recursive_get(["sample_file"], "samples.tsv")
RESULT_DIR = recursive_get(["result_dir"], "results")
mapper = recursive_get(["mapping", "mapper"], "bwa_aln")

## mapping options
run_adapter_removal = str2bool(recursive_get(["adapterremoval", "run"], True))
run_filtering = str2bool(recursive_get(["filtering", "run"], True))
remove_duplicates = recursive_get(["remove_duplicates", "run"], "adapterremoval")
run_damage_rescale = str2bool(recursive_get(["damage_rescale", "run"], False))
run_realign = str2bool(recursive_get(["realign", "run"], True))
run_compute_md = str2bool(recursive_get(["compute_md", "run"], True))

## stats
run_damage = recursive_get(["stats", "run_damage"], "False")
run_depth = str2bool(recursive_get(["stats", "run_depth"], "False"))

## retry failed jobs
memory_increment_ratio = float(recursive_get(["memory_increment_ratio"], 1))
runtime_increment_ratio = float(recursive_get(["runtime_increment_ratio"], 0))

## software versions
module_samtools = recursive_get(["envmodules", "samtools"], "")
module_bowtie2 = recursive_get(["envmodules", "bowtie2"], "")
module_bwa = recursive_get(["envmodules", "bwa"], "")
module_picard = recursive_get(["envmodules", "picard"], "")
module_gatk3 = recursive_get(["envmodules", "gatk3"], "")
module_fastqc = recursive_get(["envmodules", "fastqc"], "")
module_r = recursive_get(["envmodules", "r"], "")
module_adapterremoval = recursive_get(["envmodules", "adapterremoval"], "")
module_mapdamage = recursive_get(["envmodules", "mapdamage"], "")
module_bedtools = recursive_get(["envmodules", "bedtools"], "")
module_seqtk = recursive_get(["envmodules", "seqtk"], "")
module_qualimap = recursive_get(["envmodules", "qualimap"], "")
module_multiqc = recursive_get(["envmodules", "multiqc"], "")


##########################################################################################
## GENOME
##########################################################################################

## is at least a reference genome defined
genome = list(recursive_get(["genome"], ""))
if len(genome) == 0:
    LOGGER.error("ERROR: Reference genome is not specified (parameter 'genome')!")
    os._exit(1)

FORMAT = "%(asctime)s %(clientip)-15s %(user)-8s %(message)s"
if len(genome) > 3:
    LOGGER.info(f"{len(genome)} reference genome(s) are specified.")
else:
    LOGGER.info(f"{len(genome)} reference genome(s) are specified: {genome}")


## test if for each reference a fasta file is defined
## check the chromosome names (and adjust config)
for GENOME in genome:
    fasta = recursive_get(["genome", GENOME, "fasta"], "")
    if "fasta" == "":
        LOGGER.error(f"ERROR: Reference genome '{GENOME}' has no fasta file defined!")

    elif not os.path.isfile(fasta):
        LOGGER.error(
            f"ERROR: Reference genome '{GENOME}': Fasta file '{fasta}' does not exist!"
        )

    check_chromosome_names(GENOME)


##########################################################################################
## SAMPLE FILE
##########################################################################################

# sample_file="config/samples.tsv"
delim = recursive_get(["delim"], "\s+")
db = pd.read_csv(
    sample_file, sep=delim, comment="#", dtype=str
)  # , keep_default_na = False )

## check number of columns and column names
if len(db.columns) == 6:
    LOGGER.info(f"The sample file '{sample_file}' (in single-end format) contains:")
    paired_end = False
    collapse = False
    if not set(["ID", "Data", "MAPQ", "LB", "PL", "SM"]).issubset(db.columns):

        LOGGER.error(
            "ERROR: The column names in the sample file are wrong! Expected are for single-end reads: ID, Data, MAPQ, LB, PL, SM"
        )
        sys.exit(1)

    ## test if all fastq files exist
    for fq in db["Data"].tolist():
        if not os.path.isfile(fq):
            LOGGER.error(f"ERROR: Fastq file '{fq}' does not exist!")
            sys.exit(1)

elif len(db.columns) == 7:
    LOGGER.info(f"The sample file '{sample_file}' (in paired-end format) contains:")
    paired_end = True
    adaptrem_params = recursive_get(
        ["adapterremoval", "params"], "--minlength 30 --trimns --trimqualities"
    )
    if "--collapse" in adaptrem_params:
        collapse = True
    else:
        collapse = False
    if not set(["ID", "Data1", "Data2", "MAPQ", "LB", "PL", "SM"]).issubset(db.columns):
        LOGGER.error(
            "ERROR: The column names in the sample file are wrong! Expected are for paired-end reads: ID, Data1, Data2, MAPQ, LB, PL, SM"
        )
        sys.exit(1)

    ## test if all fastq files exist
    for fq in db["Data1"].tolist():
        if fq != fq:  # test if NaN
            LOGGER.error(f"ERROR: Fastq file '{fq}' in column Data1 is missing!")
            sys.exit(1)
        if not fq and not os.path.isfile(fq):
            LOGGER.error(f"ERROR: Fastq file '{fq}' does not exist!")
            sys.exit(1)
    for fq in db["Data2"].tolist():
        if fq == fq and not os.path.isfile(fq):
            LOGGER.error(f"ERROR: Fastq file '{fq}' in column Data2 does not exist!")
            sys.exit(1)
else:
    LOGGER.error(
        f"ERROR: The number of columns in the sample file is wrong ({len(db.columns)} columns)!"
    )
    sys.exit(1)

## check if all IDs per LB and SM are unique
all_fastq = db.groupby(["ID", "LB", "SM"])["ID"].agg(["count"]).reset_index()
if max(all_fastq["count"]) > 1:
    LOGGER.error("ERROR: The ID's have to be unique within a library and sample!")
    sys.exit(1)


## dataframe to nested dict
samples = {}
cols = [e for e in list(db.columns) if e not in ("SM", "LB", "ID")]
for index, row in db.iterrows():
    ## if key not present add new dict
    if row["SM"] not in samples:
        samples[row["SM"]] = {}
    if row["LB"] not in samples[row["SM"]]:
        samples[row["SM"]][row["LB"]] = {}
    if row["ID"] not in samples[row["SM"]][row["LB"]]:
        samples[row["SM"]][row["LB"]][row["ID"]] = {}

    ## add all remaining columns to this dict
    for col in cols:
        samples[row["SM"]][row["LB"]][row["ID"]][col] = row[col]

## print a summary of the contents
LOGGER.info(f"  - {len(samples)} samples")
LOGGER.info(f"  - {len([v for v in samples])} libraries")
LOGGER.info(f"  - {len([vv for v in samples for vv in samples[v]])} fastq files")


##########################################################################################
## TEST PARAMETERS
##########################################################################################

# add file for not trimmed data
if not run_adapter_removal:
    os.system("touch Not_trimmed")

## if the bam files have to be rescaled mapDamage has to be run
if (
    run_damage_rescale
    and recursive_get(["stats", "run_damage"], "False") != "mapDamage"
):
    LOGGER.error(
        "ERROR: To use 'damage_rescale:run' the parameter 'stats:run_damage' has to be set to 'mapDamage'!"
    )
    sys.exit(1)

## filter mappings and keep low quality mappings or not?
if run_filtering:
    save_low_qual = str2bool(recursive_get(["filtering", "save_low_qual"], "True"))
else:
    save_low_qual = False

## --------------------------------------------------------------------------------------------------
## remove duplicates
## if one wants to extract the duplicates, MarkDuplicate has to mark the duplicates in order to extract them
if remove_duplicates == "adapterremoval":
    save_duplicates = recursive_get(["remove_duplicates", "save_duplicates"], "remove")  ## remove, mark, extract
    else:  ## true,   false,false (false is default)
        save_duplicates = "remove"

    ## check that MarkDuplicates has the correct parameters
    param_dupl = recursive_get(["remove_duplicates", "params_markduplicates"], "")
    if "--REMOVE_DUPLICATES true" in param_dupl:
        if save_duplicates != "remove":
            LOGGER.info(
                "WARNING: To save/mark duplicates in MarkDuplicates the parameter 'remove_duplicates:params_markduplicates' has to contain the parameter '--REMOVE_DUPLICATES false' or not to be set! Adjusted!"
            )
            param_dupl.replace("--REMOVE_DUPLICATES true", "--REMOVE_DUPLICATES false")
            config = update_value(keys=["remove_duplicates", "params_markduplicates"], value=param_dupl)
    
    elif "--REMOVE_DUPLICATES false" in param_dupl:
        if save_duplicates == "remove":
            LOGGER.info(
                "WARNING: To remove duplicates in MarkDuplicates the parameter 'remove_duplicates:params_markduplicates' has to contain the parameter '--REMOVE_DUPLICATES true'! Adjusted!"
            )
            param_dupl.replace("--REMOVE_DUPLICATES false", "--REMOVE_DUPLICATES true")
            config = update_value(keys=["remove_duplicates", "params_markduplicates"], value=param_dupl)

    else:
        if save_duplicates == "remove":
            LOGGER.info(
                "WARNING: To remove duplicates in MarkDuplicates the parameter 'remove_duplicates:params_markduplicates' has to contain the parameter '--REMOVE_DUPLICATES true'! Adjusted!"
            )
            param_dupl = f"{param_dupl} --REMOVE_DUPLICATES true"
            config = update_value(keys=["remove_duplicates", "params_markduplicates"], value=param_dupl)

elif remove_duplicates == "dedup":
    if not paired_end:
        LOGGER.info(
            "WARNING: 'Dedup' is not recommended for single-end reads (parameter 'remove_duplicates:run')!"
        )        
    save_duplicates = recursive_get(["remove_duplicates", "save_duplicates"], "remove") 
    if save_duplicates != "remove":
        save_duplicates = "remove"
        LOGGER.info(
            "WARNING: 'Dedup' always removes duplicates (parameter 'remove_duplicates:save_duplicates')! Adjusted!"
        )
        config = update_value(keys=["remove_duplicates", "save_duplicates"], value=save_duplicates)




##########################################################################################
include: "rules/index.smk"
include: "rules/stats.smk"
include: "rules/fastq.smk"
include: "rules/library.smk"
include: "rules/sample.smk"


report: "report/workflow.rst"


##########################################################################################
##########################################################################################
localrules:
    all,
    dag,
    mapping,
    stats,  ## executed locally on a cluster


# -----------------------------------------------------------------------------#
## get all final bam files
final_bam = [
    expand(
        f"{RESULT_DIR}/03_sample/03_final_sample/01_bam/{{SM}}.{{GENOME}}.bam",
        SM=samples,
        GENOME=genome,
    ),
]
# -----------------------------------------------------------------------------#
## get all final bam files
final_bam_low_qual = [
    expand(
        f"{RESULT_DIR}/03_sample/03_final_sample/01_bam_low_qual/{{SM}}.{{GENOME}}.bam",
        SM=samples,
        GENOME=genome,
    )
    if save_low_qual
    else [],
    expand(
        "{RESULT_DIR}/03_sample/03_final_sample/01_bam_duplicate/{{SM}}.{{GENOME}}.bam",
        SM=samples,
        GENOME=genome,
    )
    if save_duplicates == "extract"
    else [],
]

# -----------------------------------------------------------------------------#
## stats that need to be computed for the mapping checkpoint
fastqc = [
    f"{RESULT_DIR}/04_stats/01_sparse_stats/01_fastq/{folder}/{SM}/{LB}/{ID}_fastqc.zip"
    for SM in samples
    for LB in samples[SM]
    for ID in samples[SM][LB]
    for folder in ["00_reads/01_files_orig", "01_trimmed/01_adapter_removal"]
]

flagstats = list(
    set(
        [
            f"{RESULT_DIR}/04_stats/01_sparse_stats/{file}_flagstat.txt"
            for GENOME in genome
            for SM in samples
            for LB in samples[SM]
            for ID in samples[SM][LB]
            for file in [
                f"01_fastq/04_final_fastq/01_bam/{SM}/{LB}/{ID}.{GENOME}",
                f"02_library/00_merged_fastq/01_bam/{SM}/{LB}.{GENOME}",
                f"02_library/03_final_library/01_bam/{SM}/{LB}.{GENOME}",
                f"03_sample/03_final_sample/01_bam/{SM}.{GENOME}",
            ]
        ]
    )
)

samtools_stats = list(
    set(
        [
            f"{RESULT_DIR}/04_stats/01_sparse_stats/{file}_stats.txt"
            for GENOME in genome
            for SM in samples
            for LB in samples[SM]
            for ID in samples[SM][LB]
            for file in [
                f"01_fastq/04_final_fastq/01_bam/{SM}/{LB}/{ID}.{GENOME}",
                f"02_library/00_merged_fastq/01_bam/{SM}/{LB}.{GENOME}",
                f"02_library/03_final_library/01_bam/{SM}/{LB}.{GENOME}",
                f"03_sample/03_final_sample/01_bam/{SM}.{GENOME}",
            ]
        ]
    )
)

lengths = [
    f"{RESULT_DIR}/04_stats/01_sparse_stats/{file}.length"
    for GENOME in genome
    for SM in samples
    for LB in samples[SM]
    for ID in samples[SM][LB]
    for file in [
        f"01_fastq/04_final_fastq/01_bam/{SM}/{LB}/{ID}.{GENOME}",
        f"02_library/03_final_library/01_bam/{SM}/{LB}.{GENOME}",
        f"03_sample/03_final_sample/01_bam/{SM}.{GENOME}",
    ]
]

lengths = list(set(lengths))

idxstats = [
    f"{RESULT_DIR}/04_stats/01_sparse_stats/{file}.idxstats"
    for GENOME in genome
    for SM in samples
    for LB in samples[SM]
    for file in [
        f"02_library/03_final_library/01_bam/{SM}/{LB}.{GENOME}",
        f"03_sample/03_final_sample/01_bam/{SM}.{GENOME}",
    ]
]

idxstats = list(set(idxstats))

sex_files = [
    f"{RESULT_DIR}/04_stats/01_sparse_stats/{file}.sex"
    for GENOME in genome
    for SM in samples
    for LB in samples[SM]
    for file in [
        f"02_library/03_final_library/01_bam/{SM}/{LB}.{GENOME}",
        f"03_sample/03_final_sample/01_bam/{SM}.{GENOME}",
    ]
    if str2bool(recursive_get(["genome", GENOME, "sex_inference", "run"], False))
]

sex_files = list(set(sex_files))

qualimap_files = [
    f"{RESULT_DIR}/04_stats/01_sparse_stats/03_sample/03_final_sample/01_bam/{SM}.{GENOME}_qualimap"
    for GENOME in genome
    for SM in samples
    if str2bool(recursive_get(["stats", "qualimap"], False))
]

qualimap_files = list(set(qualimap_files))

multiqc_files = [
    f"{RESULT_DIR}/04_stats/02_separate_tables/{GENOME}/multiqc_fastqc.html"
    for GENOME in genome
    if str2bool(recursive_get(["stats", "multiqc"], False))
]

multiqc_files = list(set(multiqc_files))


stats_mapping_checkpoint = [
    fastqc,
    flagstats,
    samtools_stats,
    lengths,
    idxstats,
    sex_files,
    get_damage(run_damage),
]


# -----------------------------------------------------------------------------#
## get the minimal final summary stats
final_stats = [
    # expand(
    #     "Missing input files for rule/04_stats/03_summary/{level}_stats.{GENOME}.csv",
    #     level=["SM", "LB", "FASTQ"],
    #     GENOME=genome
    # ),
    expand(
        f"{RESULT_DIR}/04_stats/03_summary/{{level}}_stats.csv",
        level=["SM", "LB", "FASTQ"],
    ),
    qualimap_files,
    multiqc_files,
]
# -----------------------------------------------------------------------------#

final_stat_figures = [
    get_damage(run_damage),
    expand(
        f"{RESULT_DIR}/04_stats/04_plots/{{plot_type}}.svg",
        plot_type=[
            "1_nb_reads",
            "2_mapped",
            "3_endogenous",
            "4_duplication",
            "5_AvgReadDepth",
        ],
    ),
]


##########################################################################################
## rules all
rule all:
    """
    Computes all
    """
    input:
        final_bam,
        final_bam_low_qual,
        final_stat_figures,
        final_stats,
        samtools_stats,
        stats_mapping_checkpoint,


## rules dag (just pipeline)
rule dag:
    """
    Computes just the mapping (used to get a nice DAG)
    """
    input:
        final_bam,


rule mapping:
    """
    Computes just what is needed to get the final mapping (without any stats)
    """
    input:
        final_bam,
        final_bam_low_qual,
        stats_mapping_checkpoint,


rule stats:
    """
    Computes all statistics
    """
    input:
        final_stat_figures,
        final_stats,


onsuccess:
    LOGGER.info("Wow, 'mapache' finished successfully!")
    shell("cat {log} > snakemake.run.log")
    email = recursive_get(["email"], "")
    if os.path.isfile("Not_trimmed"):
        os.system("rm Not_trimmed")
    if email != "":
        Logger.info(f"Sending email to {email}")
        shell("mail -s \"'Mapache' finished successfully\" {email} < {log}")


onerror:
    LOGGER.error("'Mapache' finished with error(s), log stored in 'snakemake.run.log'")
    shell("cat {log} > snakemake.run.log")
    email = recursive_get(["email"], "")
    if email != "":
        LOGGER.info(f"Sending email to {email}")
        shell("mail -s \"'Mapache' finished with error(s)\" {email} < {log}")
