"""
Author: Samuel Neuenschwander, Diana Ivette Cruz, Lucas Anchieri, Anna-Sapfo Malaspinas 
Affiliation: DBC, UNIL
Aim: Map ancient DNA libraries to a reference genome
Date: 28/10/2019
"""


##########################################################################################
import logging
LOGGER = logging.getLogger("snakemake.logging")


## read the config file
configfile: "config/config.yaml"


# validate(config)


include: "rules/common.smk"


## default values
SNAKE_DIR = workflow.basedir 
#LOGGER.info("Working directory is %s", {SNAKE_DIR)
sample_file = recursive_get(["sample_file"], "samples.txt")
RESULT_DIR = recursive_get(["result_dir"], "results")
mapper = recursive_get(["mapping", "mapper"], "bwa_aln")

## mapping options
run_adapter_removal = str2bool(recursive_get(["adapterremoval", "run"], True))
run_filtering = str2bool(recursive_get(["filtering", "run"], True))
run_mark_duplicates = str2bool(recursive_get(["markduplicates", "run"], True))
run_damage_rescale = str2bool(recursive_get(["damage_rescale", "run"], False))
run_realign = str2bool(recursive_get(["realign", "run"], True))
run_compute_md = str2bool(recursive_get(["compute_md", "run"], True))

## stats
run_damage = recursive_get(["stats", "run_damage"], "False")
run_depth = str2bool(recursive_get(["stats", "run_depth"], "False"))

## retry failed jobs
memory_increment_ratio = float(recursive_get(["memory_increment_ratio"], 1))
runtime_increment_ratio = float(recursive_get(["runtime_increment_ratio"], 0))

## software versions
module_samtools = recursive_get(["envmodules", "samtools"], "")
module_bowtie2 = recursive_get(["envmodules", "bowtie2"], "")
module_bwa = recursive_get(["envmodules", "bwa"], "")
module_picard = recursive_get(["envmodules", "picard"], "")
module_gatk3 = recursive_get(["envmodules", "gatk3"], "")
module_fastqc = recursive_get(["envmodules", "fastqc"], "")
module_r = recursive_get(["envmodules", "r"], "")
module_adapterremoval = recursive_get(["envmodules", "adapterremoval"], "")
module_mapdamage = recursive_get(["envmodules", "mapdamage"], "")
module_bedtools = recursive_get(["envmodules", "bedtools"], "")
module_seqtk = recursive_get(["envmodules", "seqtk"], "")


##########################################################################################
## some test to verify that the minimal requirements are met

## is at least a reference genome defined
genome = list(recursive_get(["genome"], ""))
if len(genome) == 0:
    print("ERROR: Reference genome is not specified (parameter 'genome')!")
    os._exit(1)

## test if for each reference a fasta file is defined
## check the chromosome names (and adjust config)
for GENOME in genome:
    fasta = recursive_get(["genome", GENOME, "fasta"], "")
    if "fasta" == "":
        sys.exit(f"ERROR: Reference genome '{GENOME}' has no fasta file defined!")

    elif not os.path.isfile(fasta):
        sys.exit(
            f"ERROR: Reference genome '{GENOME}': Fasta file '{fasta}' does not exist!"
        )

    check_chromosome_names(GENOME)

# add file for not trimmed data
if not run_adapter_removal:
    os.system("touch Not_trimmed")

## if the bam files have to be rescaled mapDamage has to be run
if (
    run_damage_rescale
    and recursive_get(["stats", "run_damage"], "False") != "mapDamage"
):
    sys.exit(
        "ERROR: To use 'damage_rescale:run' the parameter 'stats:run_damage' has to be set to 'mapDamage'!"
    )

## filter mappings and keep low quality mappings or not?
if run_filtering:
    save_low_qual = str2bool(recursive_get(["filtering", "save_low_qual"], "True"))
else:
    save_low_qual = False

## if one wants to extract the duplicates, MarkDuplicate has to mark the duplicates in order to extract them
if run_mark_duplicates:
    save_duplicates = recursive_get(
        ["markduplicates", "save_duplicates"], "remove"
    )  ## remove, mark, extract
else:  ## true,   false,false (false is default)
    save_duplicates = "remove"

## check that MarkDuplicates has the correct parameters
param_dupl = recursive_get(["markduplicates", "params"], "")
if "-REMOVE_DUPLICATES true" in param_dupl:
    if save_duplicates != "remove":
        print(
            "WARNING: To save/mark duplicates in MarkDuplicates the parameter 'markduplicates:params' has to contain the parameter '--REMOVE_DUPLICATES false' or not to be set! Adjusted!"
        )
        param_dupl.replace("--REMOVE_DUPLICATES true", "--REMOVE_DUPLICATES false")


elif "-REMOVE_DUPLICATES false" in param_dupl:
    if save_duplicates == "remove":
        print(
            "WARNING: To remove duplicates in MarkDuplicates the parameter 'markduplicates:params' has to contain the parameter '--REMOVE_DUPLICATES true'! Adjusted!"
        )
        param_dupl.replace("--REMOVE_DUPLICATES false", "--REMOVE_DUPLICATES true")

else:
    if save_duplicates == "remove":
        print(
            "WARNING: To remove duplicates in MarkDuplicates the parameter 'markduplicates:params' has to contain the parameter '--REMOVE_DUPLICATES true'! Adjusted!"
        )
        param_dupl = f"{param_dupl} --REMOVE_DUPLICATES true"

config = update_value(keys=["markduplicates", "params"], value=param_dupl)


##########################################################################################
## read the sample file
# sample_file="config/samples.tsv"
delim = recursive_get(["delim"], "\s+")
db = pd.read_csv(sample_file, sep=delim, comment='#')


## check number of columns and column names
if len(db.columns) == 6:
    # paired_end = 0  ## single-end library
    paired_end = False
    collapse = False
    if not set(["ID", "Data", "MAPQ", "LB", "PL", "SM"]).issubset(db.columns):

        sys.exit(
            "ERROR: The column names in the sample file are wrong! Expected are for single-end reads: ID, Data, MAPQ, LB, PL, SM"
        )

    ## test if all fastq files exist
    for fq in db["Data"].tolist():
        if not os.path.isfile(fq):
            sys.exit(f"ERROR: Fastq file '{fq}' does not exist!")


elif len(db.columns) == 7:
    paired_end = True
    adaptrem_params = config.get("adapterremoval", {}).get("params_paired_end", "")
    if "--collapse" in adaptrem_params:
        collapse = True
    # paired_end = 1  ## paired-end libraries, which are collapsed with adapterremoval
    else:
        collapse = False
    # paired_end = 2  ## paired-end libraries, which are mapped as paired-end
    if not set(["ID", "Data1", "Data2", "MAPQ", "LB", "PL", "SM"]).issubset(db.columns):
        sys.exit(
            "ERROR: The column names in the sample file are wrong! Expected are for paired-end reads: ID, Data1, Data2, MAPQ, LB, PL, SM"
        )

    ## test if all fastq files exist
    for fq in [db["Data1"].tolist(), db["Data2"].tolist()]:
        if not os.path.isfile(fq):
            sys.exit(f"ERROR: Fastq file '{fq}' does not exist!")


else:
    sys.exit(
        f"ERROR: The number of columns in the sample file is wrong ({len(db.columns)} columns)!"
    )


## check if all IDs per LB and SM are unique
all_fastq = db.groupby(["ID", "LB", "SM"])["ID"].agg(["count"]).reset_index()
if max(all_fastq["count"]) > 1:
    sys.exit("ERROR: The ID's have to be unique within a library and sample!")


## dataframe to nested dict
samples = {}
cols = [e for e in list(db.columns) if e not in ("SM", "LB", "ID")]
for index, row in db.iterrows():
    ## if key not present add new dict
    if row["SM"] not in samples:
        samples[row["SM"]] = {}
    if row["LB"] not in samples[row["SM"]]:
        samples[row["SM"]][row["LB"]] = {}
    if row["ID"] not in samples[row["SM"]][row["LB"]]:
        samples[row["SM"]][row["LB"]][row["ID"]] = {}

    ## add all remaining columns to this dict
    for col in cols:
        samples[row["SM"]][row["LB"]][row["ID"]][col] = row[col]

## Building dictionnary for libraries
all_libraries = db.groupby(["LB", "SM"])["ID"].agg(["count"]).reset_index()


##########################################################################################
include: "rules/Snakefile_index.smk"
include: "rules/Snakefile_stats.smk"
include: "rules/Snakefile_fastq.smk"
include: "rules/Snakefile_library.smk"
include: "rules/Snakefile_sample.smk"


report: "report/workflow.rst"


##########################################################################################
##########################################################################################
localrules:
    all,
    dag,
    mapping,
    stats,  ## executed locally on a cluster


# -----------------------------------------------------------------------------#
## get all final bam files
final_bam = [
    expand(
        f"{RESULT_DIR}/03_sample/03_final_sample/01_bam/{{SM}}.{{GENOME}}.bam",
        SM=samples,
        GENOME=genome,
    ),
]
# -----------------------------------------------------------------------------#
## get all final bam files
final_bam_low_qual = [
    expand(
        f"{RESULT_DIR}/03_sample/03_final_sample/01_bam_low_qual/{{SM}}.{{GENOME}}.bam",
        SM=samples,
        GENOME=genome,
    )
    if save_low_qual
    else [],
    expand(
        "{RESULT_DIR}/03_sample/03_final_sample/01_bam_duplicate/{{SM}}.{{GENOME}}.bam",
        SM=samples,
        GENOME=genome,
    )
    if save_duplicates == "extract"
    else [],
]

# -----------------------------------------------------------------------------#
## stats that need to be computed for the mapping checkpoint
fastqc = [
    f"{RESULT_DIR}/04_stats/01_sparse_stats/01_fastq/{folder}/{SM}/{LB}/{ID}_fastqc.zip"
    for SM in samples
    for LB in samples[SM]
    for ID in samples[SM][LB]
    for folder in ["00_reads/01_files_orig", "01_trimmed/01_files_trim"]
]

flagstats = list(
    set(
        [
            f"{RESULT_DIR}/04_stats/01_sparse_stats/{file}_flagstat.txt"
            for GENOME in genome
            for SM in samples
            for LB in samples[SM]
            for ID in samples[SM][LB]
            for file in [
                f"01_fastq/04_final_fastq/01_bam/{SM}/{LB}/{ID}.{GENOME}",
                f"02_library/00_merged_fastq/01_bam/{SM}/{LB}.{GENOME}",
                f"02_library/03_final_library/01_bam/{SM}/{LB}.{GENOME}",
                f"03_sample/03_final_sample/01_bam/{SM}.{GENOME}",
            ]
        ]
    )
)

lengths = [
    f"{RESULT_DIR}/04_stats/01_sparse_stats/{file}.length"
    for GENOME in genome
    for SM in samples
    for LB in samples[SM]
    for ID in samples[SM][LB]
    for file in [
        f"01_fastq/04_final_fastq/01_bam/{SM}/{LB}/{ID}.{GENOME}",
        f"02_library/03_final_library/01_bam/{SM}/{LB}.{GENOME}",
        f"03_sample/03_final_sample/01_bam/{SM}.{GENOME}",
    ]
]

lengths = list(set(lengths))

idxstats = [
    f"{RESULT_DIR}/04_stats/01_sparse_stats/{file}.idxstats"
    for GENOME in genome
    for SM in samples
    for LB in samples[SM]
    for file in [
        f"02_library/03_final_library/01_bam/{SM}/{LB}.{GENOME}",
        f"03_sample/03_final_sample/01_bam/{SM}.{GENOME}",
    ]
]

idxstats = list(set(idxstats))

sex_files = [
    f"{RESULT_DIR}/04_stats/01_sparse_stats/{file}.sex"
    for GENOME in genome
    for SM in samples
    for LB in samples[SM]
    for file in [
        f"02_library/03_final_library/01_bam/{SM}/{LB}.{GENOME}",
        f"03_sample/03_final_sample/01_bam/{SM}.{GENOME}",
    ]
    if str2bool(recursive_get(["genome", GENOME, "sex_inference", "run"], False))
]

sex_files = list(set(sex_files))

stats_mapping_checkpoint = [
    fastqc,
    flagstats,
    lengths,
    idxstats,
    sex_files,
    get_damage(run_damage),
]


# -----------------------------------------------------------------------------#
## get the minimal final summary stats
final_stats = [
    # expand(
    #     "Missing input files for rule/04_stats/03_summary/{level}_stats.{GENOME}.csv",
    #     level=["SM", "LB", "FASTQ"],
    #     GENOME=genome
    # ),
    expand(
        f"{RESULT_DIR}/04_stats/03_summary/{{level}}_stats.csv",
        level=["SM", "LB", "FASTQ"],
    )
]
# -----------------------------------------------------------------------------#

final_stat_figures = [
    get_damage(run_damage),
    expand(
        f"{RESULT_DIR}/04_stats/04_plots/{{plot_type}}.svg",
        plot_type=[
            "1_nb_reads",
            "2_mapped",
            "3_endogenous",
            "4_duplication",
            "5_AvgReadDepth",
        ],
    ),
]


##########################################################################################
## rules all
rule all:
    """
    Computes all
    """
    input:
        final_bam,
        final_bam_low_qual,
        final_stat_figures,
        final_stats,


## rules dag (just pipeline)
rule dag:
    """
    Computes just the mapping (used to get a nice DAG)
    """
    input:
        final_bam,


rule mapping:
    """
    Computes just what is needed to get the final mapping (without any stats)
    """
    input:
        final_bam,
        final_bam_low_qual,
        stats_mapping_checkpoint,


rule stats:
    """
    Computes all statistics
    """
    input:
        final_stat_figures,
        final_stats,


onsuccess:
    print("Wow, 'mapache' finished successfully!")
    shell("cat {log} > snakemake.run.log")
    email = recursive_get(["email"], "")
    if os.path.isfile("Not_trimmed"):
        os.system("rm Not_trimmed")
    if email != "":
        print(f"sending email to {email}")
        shell(
            "mail -s \"Snakemake workflow 'mapache' finished successfully\" {email} < {log}"
        )


onerror:
    print("An error occurred, stored to 'snakemake.run.log'")
    shell("cat {log} > snakemake.run.log")
    email = recursive_get(["email"], "")
    if email != "":
        print(f"sending email to {email}")
        shell(
            "mail -s \"Snakemake workflow 'mapache' finished with error(s)\" {email} < {log}"
        )
