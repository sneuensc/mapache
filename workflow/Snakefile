"""
Author: Samuel Neuenschwander, Diana Ivette Cruz, Lucas Anchieri, Anna-Sapfo Malaspinas 
Affiliation: DBC, UNIL
Aim: Map ancient DNA libraries to a reference genome
Date: 28/10/2019
"""


##########################################################################################
import logging

LOGGER = logging.getLogger("snakemake.logging")
# logging.basicConfig(filename="example.log", encoding="utf-8", level=logging.DEBUG)
# logging.basicConfig(format="%(levelname)s %(message)s")


## read the config file
configfile: "config/config.yaml"


include: "rules/common.smk"


RESULT_DIR = recursive_get(["result_dir"], "results")


## retry failed jobs
memory_increment_ratio = float(recursive_get(["memory_increment_ratio"], 1))
runtime_increment_ratio = float(recursive_get(["runtime_increment_ratio"], 0))

## software versions
module_samtools = recursive_get(["envmodules", "samtools"], "")
module_bowtie2 = recursive_get(["envmodules", "bowtie2"], "")
module_bwa = recursive_get(["envmodules", "bwa"], "")
module_picard = recursive_get(["envmodules", "picard"], "")
module_gatk3 = recursive_get(["envmodules", "gatk3"], "")
module_fastqc = recursive_get(["envmodules", "fastqc"], "")
module_r = recursive_get(["envmodules", "r"], "")
module_adapterremoval = recursive_get(["envmodules", "adapterremoval"], "")
module_mapdamage = recursive_get(["envmodules", "mapdamage"], "")
module_bedtools = recursive_get(["envmodules", "bedtools"], "")
module_seqtk = recursive_get(["envmodules", "seqtk"], "")
module_qualimap = recursive_get(["envmodules", "qualimap"], "")
module_multiqc = recursive_get(["envmodules", "multiqc"], "")
module_dedup = recursive_get(["envmodules", "dedup"], "")


##########################################################################################
## REFERENCE GENOME
##########################################################################################
## is at least a reference genome defined
genome = list(recursive_get(["genome"], ""))
if len(genome) == 0:
    LOGGER.error("ERROR: Reference genome is not specified (parameter 'genome')!")
    os._exit(1)

if len(genome) == 1:
    LOGGER.info(f"REFERENCE GENOME: 1 genome is specified:")
elif len(genome) < 4:
    LOGGER.info(f"REFERENCE GENOME: {len(genome)} genomes are specified:")
else:
    LOGGER.info(f"REFERENCE GENOME: {len(genome)} genomes are specified.")

## test fasta and chomosomne names
for GENOME in genome:
    check_chromosome_names(GENOME, len(genome) <= 3)


##########################################################################################
## SAMPLE FILE
##########################################################################################
# sample_file="config/samples.tsv"
sample_file = recursive_get(["sample_file"], "config/samples.tsv")
db = pd.read_csv(sample_file, sep="\t", comment="#", dtype=str)

## check number of columns and column names
if len(db.columns) == 6:
    paired_end = False
    collapse = False
    colnames = ["ID", "Data", "MAPQ", "LB", "PL", "SM"]
    if not set(colnames).issubset(db.columns):
        LOGGER.error(
            f"ERROR: The column names in the sample file are wrong! Expected are for single-end reads {colnames}!"
        )
        sys.exit(1)

    ## test if all fastq files exist
    for fq in db["Data"].tolist():
        if not os.path.isfile(fq):
            LOGGER.error(f"ERROR: Fastq file '{fq}' does not exist!")
            sys.exit(1)

elif len(db.columns) == 7:
    paired_end = True
    adaptrem_params = recursive_get(
        ["adapterremoval", "params"], "--minlength 30 --trimns --trimqualities"
    )
    if "--collapse" in adaptrem_params:
        collapse = True
    else:
        collapse = False
    colnames = ["ID", "Data1", "Data2", "MAPQ", "LB", "PL", "SM"]
    if not set(colnames).issubset(db.columns):
        LOGGER.error(
            f"ERROR: The column names in the sample file are wrong! Expected are for paired-end reads {colnames}!"
        )
        sys.exit(1)

    ## test if all fastq files exist
    for fq in db["Data1"].tolist():
        if fq != fq:  # test if NaN
            LOGGER.error(f"ERROR: Fastq file '{fq}' in column Data1 is missing!")
            sys.exit(1)
        if not fq and not os.path.isfile(fq):
            LOGGER.error(f"ERROR: Fastq file '{fq}' does not exist!")
            sys.exit(1)

    ## test if all fastq files exist or are NaN
    for fq in db["Data2"].tolist():
        if fq == fq and not os.path.isfile(fq):
            LOGGER.error(f"ERROR: Fastq file '{fq}' in column Data2 does not exist!")
            sys.exit(1)
else:
    LOGGER.error(
        f"ERROR: The number of columns in the sample file is wrong ({len(db.columns)} columns)!"
    )
    sys.exit(1)

## --------------------------------------------------------------------------------------------------
## check if all IDs per LB and SM are unique
all_fastq = db.groupby(["ID", "LB", "SM"])["ID"].agg(["count"]).reset_index()
if max(all_fastq["count"]) > 1:
    LOGGER.error(
        f"ERROR: The ID's {all_fastq['ID']} are not uniq within library and sample!"
    )
    sys.exit(1)


## --------------------------------------------------------------------------------------------------
## dataframe to nested dict
samples = {}
cols = [e for e in list(db.columns) if e not in ("SM", "LB", "ID")]
for index, row in db.iterrows():
    ## if key not present add new dict
    if row["SM"] not in samples:
        samples[row["SM"]] = {}
    if row["LB"] not in samples[row["SM"]]:
        samples[row["SM"]][row["LB"]] = {}
    if row["ID"] not in samples[row["SM"]][row["LB"]]:
        samples[row["SM"]][row["LB"]][row["ID"]] = {}

    ## add all remaining columns to this dict
    for col in cols:
        samples[row["SM"]][row["LB"]][row["ID"]][col] = row[col]


## --------------------------------------------------------------------------------------------------
## print a summary of the contents
if paired_end:
    LOGGER.info(f"SAMPLE FILE: sample file '{sample_file}' is in paired-end format:")
    LOGGER.info(f"  - {len(samples)} samples")
    LOGGER.info(f"  - {len([l for s in samples.values() for l in s])} libraries")
    tmp = [i["Data2"] for s in samples.values() for l in s.values() for i in l.values()]
    LOGGER.info(
        f"  - {len([x for x in tmp if str(x) != 'nan'])} paired-end fastq files"
    )
    nb = len([x for x in tmp if str(x) == "nan"])
    if nb:
        LOGGER.info(f"  - {nb} single-end fastq files")
else:
    LOGGER.info(f"SAMPLE FILE: sample file '{sample_file}' is in single-end format:")
    LOGGER.info(f"  - {len(samples)} samples")
    LOGGER.info(f"  - {len([l for s in samples.values() for l in s])} libraries")
    LOGGER.info(
        f"  - {len([i for s in samples.values() for l in s.values() for i in l])} single-end fastq files"
    )


##########################################################################################
##  PARAMETERS
##########################################################################################
## --------------------------------------------------------------------------------------------------
## subsampling
run_subsampling = str2bool(
    recursive_get_and_test(["subsampling", "run"], ["False", "True"])
)
subsampling_number = float(recursive_get(["subsampling", "number"], 0))
if subsampling_number == 0:
    run_subsampling = False


## --------------------------------------------------------------------------------------------------
## adapterremoval
run_adapter_removal = str2bool(
    recursive_get_and_test(["adapterremoval", "run"], ["True", "False"])
)

# add file for not trimmed data
if not run_adapter_removal:
    os.system("touch Not_trimmed")


## --------------------------------------------------------------------------------------------------
## mapping
mapper = recursive_get_and_test(
    ["mapping", "mapper"], ["bwa_aln", "bwa_mem", "bowtie2"]
)


## --------------------------------------------------------------------------------------------------
## filtering
run_filtering = str2bool(
    recursive_get_and_test(["filtering", "run"], ["True", "False"])
)

if run_filtering:
    save_low_qual = str2bool(
        recursive_get_and_test(["filtering", "save_low_qual"], ["True", "False"])
    )
else:
    save_low_qual = False


## --------------------------------------------------------------------------------------------------
## removing duplicates
remove_duplicates = recursive_get_and_test(
    ["remove_duplicates", "run"], ["markduplicates", "dedup", "False"]
)

if remove_duplicates == "dedup" and not paired_end:
    LOGGER.info(
        "WARNING: 'DeDup' is not recommended for single-end reads (parameter 'remove_duplicates:run')!"
    )


## --------------------------------------------------------------------------------------------------
## rescaling damage
run_damage_rescale = str2bool(
    recursive_get_and_test(["damage_rescale", "run"], ["False", "True"])
)
if (
    run_damage_rescale
    and recursive_get(["stats", "run_damage"], "False") != "mapDamage"
):
    LOGGER.error(
        "ERROR: To use 'damage_rescale:run' the parameter 'stats:run_damage' has to be set to 'mapDamage'!"
    )
    sys.exit(1)


## --------------------------------------------------------------------------------------------------
## realigning
run_realign = str2bool(recursive_get_and_test(["realign", "run"], ["True", "False"]))


## --------------------------------------------------------------------------------------------------
## recomputing md-flag
run_compute_md = str2bool(
    recursive_get_and_test(["compute_md", "run"], ["True", "False"])
)


## --------------------------------------------------------------------------------------------------
# print a summary of the workflow
LOGGER.info("WORKFLOW:")
if run_subsampling:
    if subsampling_number < 1:
        LOGGER.info(f"  - Subsampling {100 * subsampling_number}% of the reads")
    else:
        LOGGER.info(f"  - Subsampling {subsampling_number} reads per fastq file")

if run_adapter_removal:
    if collapse:
        LOGGER.info(
            f"  - Removing adapters with AdapterRemoval and collapsing paired-end reads"
        )
    else:
        LOGGER.info(f"  - Removing adapters with AdapterRemoval")

LOGGER.info(f"  - Mapping with {mapper}")

LOGGER.info(f"  - Sorting bam file")

if run_filtering:
    if save_low_qual:
        LOGGER.info(f"  - Filtering and keeping seperately low quality/unmapped reads")
    else:
        LOGGER.info(f"  - Filtering and removing low quality/unmapped reads")

if remove_duplicates != "False":
    if remove_duplicates == "markduplicates":
        LOGGER.info(f"  - Removing duplicates with MarkDuplicates")
    elif markduplicates == "dedup":
        LOGGER.info(f"  - Removing duplicates with DeDup")
    else:
        LOGGER.info(f"  - Removing duplicates with {remove_duplicates}")

if run_damage_rescale:
    LOGGER.info(f"  - Rescaling damage with MapDamage2")

if run_realign:
    LOGGER.info(f"  - Realigning indels with GATK v3.8")

if run_compute_md:
    LOGGER.info(f"  - Recomputing md flag")


##########################################################################################
##  STATISTICS
##########################################################################################
## damage
run_damage = recursive_get_and_test(
    ["stats", "run_damage"], ["False", "bamdamage", "mapDamage"]
)

run_sex_inference = recursive_get_and_test(
    ["genome", GENOME, "sex_inference", "run"], ["False", "True"]
)

run_qualimap = recursive_get_and_test(["stats", "qualimap"], ["False", "True"])

run_multiqc = recursive_get_and_test(["stats", "multiqc"], ["False", "True"])

## --------------------------------------------------------------------------------------------------
# print a summary of the stats
LOGGER.info("STATISTICS:")
LOGGER.info(f"  - Mapping statistics tables (SM_stats.csv, LB_stats.csv, FASTQ_stats.csv)")

if run_sex_inference:
    LOGGER.info(f"  - Inferring sex")

if run_damage != "False":
    if run_damage == "mapDamage":
        LOGGER.info(
            f"  - Inferring damage and read length with MapDamage2 on all alignments"
        )

    fraction = recursive_get(["stats", "bamdamage_fraction"], 0)
    if fraction == 0:
        LOGGER.info(
            f"  - Inferring damage and read length with bamdamge on all alignments"
        )
    elif fraction < 1:
        LOGGER.info(
            f"  - Inferring damage and read length with bamdamge on {100 * fraction}% of the alignments"
        )
    else:
        LOGGER.info(
            f"  - Inferring damage and read length with bamdamge on {fraction} alignments"
        )

if run_qualimap:
    LOGGER.info(f"  - Qualimap report: report on final bam file of the fastq, library and sample level")

if run_multiqc:
    LOGGER.info(f"  - Mulitqc report: HTML combining all detailed statistics")

LOGGER.info(
    f"  => Snakemake report: run 'snakemake --report report.html' after finalisation of the run to get the report"
)


##########################################################################################
include: "rules/index.smk"
include: "rules/stats.smk"
include: "rules/fastq.smk"
include: "rules/library.smk"
include: "rules/sample.smk"


report: "report/workflow.rst"


##########################################################################################
##########################################################################################
localrules:
    all,
    dag,
    mapping,
    stats,  ## executed locally on a cluster


# -----------------------------------------------------------------------------#
## get all final bam files
final_bam = [
    expand(
        f"{RESULT_DIR}/03_sample/03_final_sample/01_bam/{{SM}}.{{GENOME}}.bam",
        SM=samples,
        GENOME=genome,
    ),
]
# -----------------------------------------------------------------------------#
## get all final bam files
final_bam_low_qual = [
    expand(
        f"{RESULT_DIR}/03_sample/03_final_sample/01_bam_low_qual/{{SM}}.{{GENOME}}.bam",
        SM=samples,
        GENOME=genome,
    )
    if save_low_qual
    else [],
]

# -----------------------------------------------------------------------------#
## stats that need to be computed for the mapping checkpoint
fastqc = [
    f"{RESULT_DIR}/04_stats/01_sparse_stats/01_fastq/{folder}/{SM}/{LB}/{ID}_fastqc.zip"
    for SM in samples
    for LB in samples[SM]
    for ID in samples[SM][LB]
    for folder in ["00_reads/01_files_orig", "01_trimmed/01_adapter_removal"]
]

flagstats = list(
    set(
        [
            f"{RESULT_DIR}/04_stats/01_sparse_stats/{folder}/01_bam/{SM}/{LB}/{ID}.{GENOME}_flagstat.txt"
            for GENOME in genome
            for SM in samples
            for LB in samples[SM]
            for ID in samples[SM][LB]
            for folder in [
                f"01_fastq/04_final_fastq",
                f"02_library/00_merged_fastq",
                f"02_library/03_final_library",
                f"03_sample/03_final_sample/",
            ]
        ]
    )
)

samtools_stats = list(
    set(
        [
            f"{RESULT_DIR}/04_stats/01_sparse_stats/{folder}/01_bam/{SM}/{LB}/{ID}.{GENOME}_stats.txt"
            for GENOME in genome
            for SM in samples
            for LB in samples[SM]
            for ID in samples[SM][LB]
            for folder in [
                f"01_fastq/04_final_fastq",
                f"02_library/00_merged_fastq",
                f"02_library/03_final_library",
                f"03_sample/03_final_sample",
            ]
        ]
    )
)

lengths = [
    f"{RESULT_DIR}/04_stats/01_sparse_stats/{folder}/{SM}/{LB}/{ID}.{GENOME}.length"
    for GENOME in genome
    for SM in samples
    for LB in samples[SM]
    for ID in samples[SM][LB]
    for folder in [
        f"01_fastq/04_final_fastq",
        f"02_library/03_final_library",
        f"03_sample/03_final_sample",
    ]
]

lengths = list(set(lengths))

idxstats = [
    f"{RESULT_DIR}/04_stats/01_sparse_stats/{folder}/01_bam/{SM}/{LB}.{GENOME}.idxstats"
    for GENOME in genome
    for SM in samples
    for LB in samples[SM]
    for folder in [
        f"02_library/03_final_library",
        f"03_sample/03_final_sample",
    ]
]

idxstats = list(set(idxstats))

sex_files = [
    f"{RESULT_DIR}/04_stats/01_sparse_stats/{folder}/01_bam/{SM}/{LB}.{GENOME}.sex"
    for GENOME in genome
    for SM in samples
    for LB in samples[SM]
    for folder in [
        f"02_library/03_final_library",
        f"03_sample/03_final_sample",
    ]
    if recursive_get_and_test(["genome", GENOME, "sex_inference", "run"], ["False", "True"])
]

sex_files = list(set(sex_files))
print(sex_files)

qualimap_files = [
    f"{RESULT_DIR}/04_stats/01_sparse_stats/03_sample/03_final_sample/01_bam/{SM}.{GENOME}_qualimap"
    for GENOME in genome
    for SM in samples
    if run_qualimap
]

qualimap_files = list(set(qualimap_files))

multiqc_files = [
    f"{RESULT_DIR}/04_stats/02_separate_tables/{GENOME}/multiqc_fastqc.html"
    for GENOME in genome
    if run_multiqc
]

multiqc_files = list(set(multiqc_files))


stats_mapping_checkpoint = [
    fastqc,
    flagstats,
    samtools_stats,
    lengths,
    idxstats,
    sex_files,
    get_damage(run_damage),
]


# -----------------------------------------------------------------------------#
## get the minimal final summary stats
final_stats = [
    # expand(
    #     "Missing input files for rule/04_stats/03_summary/{level}_stats.{GENOME}.csv",
    #     level=["SM", "LB", "FASTQ"],
    #     GENOME=genome
    # ),
    expand(
        f"{RESULT_DIR}/04_stats/03_summary/{{level}}_stats.csv",
        level=["SM", "LB", "FASTQ"],
    ),
    qualimap_files,
    multiqc_files,
]
# -----------------------------------------------------------------------------#

final_stat_figures = [
    get_damage(run_damage),
    expand(
        f"{RESULT_DIR}/04_stats/04_plots/{{plot_type}}.svg",
        plot_type=[
            "1_nb_reads",
            "2_mapped",
            "3_endogenous",
            "4_duplication",
            "5_AvgReadDepth",
        ],
    ),
]


##########################################################################################
## rules all
rule all:
    """
    Computes all
    """
    input:
        final_bam,
        final_bam_low_qual,
        final_stat_figures,
        final_stats,
        samtools_stats,
        stats_mapping_checkpoint,
    message:
        "--- RUNNING ALL"


## rules dag (just pipeline)
rule dag:
    """
    Computes just the mapping (used to get a nice DAG)
    """
    input:
        final_bam,
    message:
        "--- RUNNING DAG"


rule mapping:
    """
    Computes just what is needed to get the final mapping (without any stats)
    """
    input:
        final_bam,
        final_bam_low_qual,
        stats_mapping_checkpoint,
    message:
        "--- RUNNING MAPPING"


rule stats:
    """
    Computes all statistics
    """
    input:
        final_stat_figures,
        final_stats,
    message:
        "--- RUNNING STATS"


onsuccess:
    LOGGER.info("Wow, 'mapache' finished successfully!")
    shell("cat {log} > snakemake.run.log")
    email = recursive_get(["email"], "")
    if os.path.isfile("Not_trimmed"):
        os.system("rm Not_trimmed")
    if email != "":
        Logger.info(f"Sending email to {email}")
        shell("mail -s \"'Mapache' finished successfully\" {email} < {log}")


onerror:
    LOGGER.error("'Mapache' finished with error(s), log stored in 'snakemake.run.log'")
    shell("cat {log} > snakemake.run.log")
    email = recursive_get(["email"], "")
    if email != "":
        LOGGER.info(f"Sending email to {email}")
        shell("mail -s \"'Mapache' finished with error(s)\" {email} < {log}")
