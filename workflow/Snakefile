"""
Author: Samuel Neuenschwander, Diana Ivette Cruz, Lucas Anchieri, Anna-Sapfo Malaspinas 
Affiliation: DBC, UNIL
Aim: Map ancient DNA libraries to a reference genome
Date: 28/10/2019
"""


##########################################################################################
## read the config file
configfile: "config/config.yaml"


# validate(config)


include: "rules/common.smk"


## default values
sample_file = get_param1("sample_file", "samples.txt")
email = get_param1("email", "")
delim = get_param1("delim", "\s+")
mapper = get_param2("mapping", "mapper", "bwa_aln")

## mapping options
run_adapter_removal = str2bool(get_param2("adapterremoval", "run", True))
run_filtering = str2bool(get_param2("filtering", "run", True))
run_mark_duplicates = str2bool(get_param2("markduplicates", "run", True))
run_mapDamage_rescale = str2bool(get_param2("mapdamage", "run_rescale", False))
run_realign = str2bool(get_param2("realign", "run", True))
run_compute_md = str2bool(get_param2("compute_md", "run", True))

## stats
run_damage = get_param2("stats", "run_damage", "False")
run_depth = str2bool(get_param2("stats", "run_depth", "False"))
run_bammds = str2bool(get_param2("stats", "run_bammds", "False"))

## retry failed jobs
memory_increment_ratio = float(get_param1("memory_increment_ratio", 1))
runtime_increment_ratio = float(get_param1("runtime_increment_ratio", 0))

## software versions
module_samtools = get_param2("envmodules", "samtools", "")
module_bowtie2 = get_param2("envmodules", "bowtie2", "")
module_bwa = get_param2("envmodules", "bwa", "")
module_picard = get_param2("envmodules", "picard", "")
module_gatk3 = get_param2("envmodules", "gatk3", "")
module_fastqc = get_param2("envmodules", "fastqc", "")
module_r = get_param2("envmodules", "r", "")
module_adapterremoval = get_param2("envmodules", "adapterremoval", "")
module_mapdamage = get_param2("envmodules", "mapdamage", "")
module_multiqc = get_param2("envmodules", "multiqc", "")
module_bedtools = get_param2("envmodules", "bedtools", "")
module_seqtk = get_param2("envmodules", "seqtk", "")



##########################################################################################
## some test to verify that the minimal requirements are met

## is at least a reference genome defined
genome = list(config["genome"])
if len(genome) == 0:
    print("ERROR: Reference genome is not specified (parameter genome)!")
    os._exit(1)

## test if for each reference a fasta file is defined
## check the chromosome names (and adjust config)
save_sex = False
save_MT = False
for GENOME in genome:
    if "fasta" not in list(config["genome"][GENOME]):
        print("ERROR: Each reference genome requires a fasta file!")
        os._exit(1)

    check_chromosome_names(GENOME)

    if (get_param3("genome", GENOME, "femaleChr", "") != "") and (
        get_param3("genome", GENOME, "femaleChr", "") != ""
    ):
        save_sex = True

    if get_param3("genome", GENOME, "mtChr", "") != "":
        save_MT = True


## if the bam files have to be rescaled mapDamage has to be run
if run_mapDamage_rescale and get_param2("stats", "run_damage", "False") != "mapDamage":
    print(
        "ERROR: To use 'mapdamage:run_rescale' the parameter 'stats:run_damage' has to be set to 'mapDamage'!"
    )
    os._exit(0)

## filter mappings and keep low quality mappings or not?
if run_filtering:
    save_low_qual = str2bool(get_param2("filtering", "save_low_qual", "True"))
else:
    save_low_qual = False

## if one wants to extract the duplicates, MarkDuplicate has to mark the duplicates in order to extract them
if run_mark_duplicates:
    save_duplicates = get_param2(
        "markduplicates", "save_duplicates", "remove"
    )  ## remove, mark, extract
else:  ## true,   false,false (false is default)
    save_duplicates = "remove"

## check that MarkDuplicates has the correct parameters
param_dupl = get_param2("markduplicates", "params", "")
if "REMOVE_DUPLICATES=true" in param_dupl:
    if save_duplicates != "remove":
        print(
            "WARNING: To save/mark duplicates in MarkDuplicates the parameter 'markduplicates:params' has to cotain the parameter 'REMOVE_DUPLICATES=false' or not to be set! Adjusted!"
        )
        param_dupl.replace("REMOVE_DUPLICATES=true", "REMOVE_DUPLICATES=false")
        set_param2("markduplicates", "params", param_dupl)

elif "REMOVE_DUPLICATES=false" in param_dupl:
    if save_duplicates == "remove":
        print(
            "WARNING: To remove duplicates in MarkDuplicates the parameter 'markduplicates:params' has to cotain the parameter 'REMOVE_DUPLICATES=true'! Adjusted!"
        )
        param_dupl.replace("REMOVE_DUPLICATES=false", "REMOVE_DUPLICATES=true")
        set_param2("markduplicates", "params", param_dupl)
else:
    if save_duplicates == "remove":
        print(
            "WARNING: To remove duplicates in MarkDuplicates the parameter 'markduplicates:params' has to cotain the parameter 'REMOVE_DUPLICATES=true'! Adjusted!"
        )
        param_dupl = f"{param_dupl} REMOVE_DUPLICATES=true"
        set_param2("markduplicates", "params", param_dupl)


##########################################################################################
## read the sample file
# sample_file="sample_file.txt"
db = pd.read_csv(sample_file, sep=delim)


## check number of columns and column names
if len(db.columns) == 6:
    paired_end = 0  ## single-end library
    if not set(["ID", "Data", "MAPQ", "LB", "PL", "SM"]).issubset(db.columns):
        print(
            "ERROR: The column names in the sample file are wrong! Expected are for single-end reads: ID, Data, MAPQ, LB, PL, SM"
        )
        os._exit(0)
elif len(db.columns) == 7:
    adaptrem_params = (
        config["adaptrem_params"] if "adaptrem_params" in list(config) else ""
    )
    if "--collapse" in adaptrem_params:
        paired_end = 1  ## paired-end libraries, which are collapsed with adapterremoval
    else:
        paired_end = 2  ## paired-end libraries, which are mapped as paired-end

    if not set(["ID", "Data1", "Data2", "MAPQ", "LB", "PL", "SM"]).issubset(db.columns):
        print(
            "ERROR: The column names in the sample file are wrong! Expected are for paired-end reads: ID, Data1, Data2, MAPQ, LB, PL, SM"
        )
        os._exit(0)
else:
    print(
        f"ERROR: The number of columns in the sample file is wrong ({len(db.columns)} columns)!"
    )
    os._exit(0)


## check if all IDs per LB and SM are unique
all_fastq = db.groupby(["ID", "LB", "SM"])["ID"].agg(["count"]).reset_index()
if max(all_fastq["count"]) > 1:
    print("ERROR: The ID's have to be unique within a library and sample!")
    os._exit(0)


## dataframe to nested dict
samples = {}
cols = [e for e in list(db.columns) if e not in ("SM", "LB", "ID")]
for index, row in db.iterrows():
    ## if key not present add new dict
    if row["SM"] not in samples:
        samples[row["SM"]] = {}
    if row["LB"] not in samples[row["SM"]]:
        samples[row["SM"]][row["LB"]] = {}
    if row["ID"] not in samples[row["SM"]][row["LB"]]:
        samples[row["SM"]][row["LB"]][row["ID"]] = {}

    ## add all remaining columns to this dict
    for col in cols:
        samples[row["SM"]][row["LB"]][row["ID"]][col] = row[col]

## Building dictionnary for libraries
all_libraries = db.groupby(["LB", "SM"])["ID"].agg(["count"]).reset_index()


##########################################################################################
include: "rules/Snakefile_index.smk"
include: "rules/Snakefile_stats.smk"
include: "rules/Snakefile_fastq.smk"
include: "rules/Snakefile_library.smk"
include: "rules/Snakefile_sample.smk"


report: "report/workflow.rst"


##########################################################################################
##########################################################################################
localrules:
    all,
    dag,
    mapping,
    stats,  ## executed locally on a cluster

#-----------------------------------------------------------------------------#
## get all final bam files
final_bam = [
    expand(
        "results/03_sample/03_final_sample/01_bam/{SM}.{GENOME}.bam",
        SM=samples,
        GENOME=genome,
    ),
]
#-----------------------------------------------------------------------------#
## get all final bam files
final_bam_low_qual = [
    expand(
        "results/03_sample/03_final_sample/01_bam_low_qual/{SM}.{GENOME}.bam",
        SM=samples,
        GENOME=genome,
    )
    if save_low_qual
    else [],
    expand(
        "results/03_sample/03_final_sample/01_bam_duplicate/{SM}.{GENOME}.bam",
        SM=samples,
        GENOME=genome,
    )
    if save_duplicates == "extract"
    else [],
]

#-----------------------------------------------------------------------------#
## stats that need to be computed for the mapping checkpoint
fastqc =    [
    f"results/04_stats/01_sparse_stats/01_fastq/{folder}/{SM}/{LB}/{ID}_fastqc.zip"
    for SM in samples for LB in samples[SM] for ID in samples[SM][LB]
    for folder in ["00_reads/01_files_orig", "01_trimmed/01_files_trim"]
]

flagstats = [
    f"results/04_stats/01_sparse_stats/{file}_flagstat.txt"
    for GENOME in genome for SM in samples for LB in samples[SM] for ID in samples[SM][LB]
    for file in 
        [f"01_fastq/04_final_fastq/01_bam/{SM}/{LB}/{ID}.{GENOME}",
        f"02_library/00_merged_fastq/01_bam/{SM}/{LB}.{GENOME}",
        f"02_library/03_final_library/01_bam/{SM}/{LB}.{GENOME}",
        f"03_sample/03_final_sample/01_bam/{SM}.{GENOME}"]
]

flagstats = list(set(flagstats))

lengths = [
    f"results/04_stats/01_sparse_stats/{file}.length"
    for GENOME in genome for SM in samples for LB in samples[SM] for ID in samples[SM][LB]
    for file in [
        f"01_fastq/04_final_fastq/01_bam/{SM}/{LB}/{ID}.{GENOME}",
        f"02_library/03_final_library/01_bam/{SM}/{LB}.{GENOME}",
        f"03_sample/03_final_sample/01_bam/{SM}.{GENOME}"
    ]
]

lengths = list(set(lengths))

idxstats = [
    f"results/04_stats/01_sparse_stats/{file}.idxstats"
    for GENOME in genome for SM in samples for LB in samples[SM] 
    for file in [
        f"02_library/03_final_library/01_bam/{SM}/{LB}.{GENOME}",
        f"03_sample/03_final_sample/01_bam/{SM}.{GENOME}"
    ]
]

idxstats = list(set(idxstats))

sex_files = [
    f"results/04_stats/01_sparse_stats/{file}.sex"
    for GENOME in genome for SM in samples for LB in samples[SM] 
    for file in [
        f"02_library/03_final_library/01_bam/{SM}/{LB}.{GENOME}",
        f"03_sample/03_final_sample/01_bam/{SM}.{GENOME}"
    ]
    if config["genome"][GENOME].get("sex_inference", {}).get("run", False)
]

sex_files = list(set(sex_files))

stats_mapping_checkpoint = [ fastqc, flagstats, lengths, idxstats, sex_files, get_damage(run_damage) ]

#-----------------------------------------------------------------------------#
## get the minimal final summary stats
final_stats = [
    expand(
        "results/04_stats/03_summary/{level}_stats.{GENOME}.csv",
        level=["SM", "LB", "FASTQ"],
        GENOME=genome,
    ),
    expand(
        "results/04_stats/03_summary/{level}_stats.csv",
        level=["SM", "LB", "FASTQ"],
        GENOME=genome,
    )
]
#-----------------------------------------------------------------------------#

final_stat_figures = [
    get_damage(run_damage),
    expand(
        "results/04_stats/04_plots/{plot_type}.svg",
        #GENOME=genome,
        plot_type = ["1_nb_reads", "2_mapped", "3_endogenous", "4_duplication", "5_AvgReadDepth"]
    )
]

##########################################################################################
## rules all
rule all:
    """
    Computes all
    """
    input:
        final_bam,
        final_bam_low_qual,
        final_stat_figures,
        final_stats,


## rules dag (just pipeline)
rule dag:
    """
    Computes just the mapping (used to get a nice DAG)
    """
    input:
        final_bam,


rule mapping:
    """
    Computes just what is needed to get the final mapping (without any stats)
    """
    input:
        final_bam,
        final_bam_low_qual,
        stats_mapping_checkpoint


rule stats:
    """
    Computes all statistics
    """
    input:
        final_stat_figures,


onsuccess:
    print("Wow, 'snakemake_aDNA_mapping' finished successfully!")
    if email != "":
        print(f"sending email to {email}")
        shell(
            "mail -s \"Snakemake workflow 'snakemake_aDNA_mapping' finished successfully\" {email} < {log}"
        )


onerror:
    print("An error occurred, stored to 'snakemake.run.log'")
    shell("cat {log} > snakemake.run.log")
    if email != "":
        print(f"sending email to {email}")
        shell(
            "mail -s \"Snakemake workflow 'snakemake_aDNA_mapping' finished with error(s)\" {email} < {log}"
        )


##########################################################################################
# rule check_software_modules:
#     """
#     Checks if software modules are available
#     """
#     output:
#         touch("logs/check_software_modules.done")
#     params:
#         modules=RequiredModules()
#     message: "--- Loading required modules"
#     threads: 1
#     shell: "{params.modules}"
##########################################################################################
##########################################################################################
